# Project 3: A Data Warehouse for Sparkify

## Problem discussion
A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in **S3**, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

In this project we will build an ETL pipeline to move and transform that data from S3 into a **Redshift** database with dimensional tables for the analytics team to continue finding insights in what songs their users are listening to. The database will be optimized to find the **top songs** of each *region* on different *periods*.

## Solution
The solution will consist of four python files:

- **create_tables.py**: a python script to create the tables. This will be executed first and once.
- **etl.py**: a python file that when called as main will load all the .json files and insert data onto staging tables, then data from staging tables will be processed and loaded onto the analytic tables.
- **sql_queries.py**: a python file with strings defining the SQL queries that will be imported and used by the other two files.
- **get_error_log.py**: a python file that downloads the error logs generated by the cluster, in case some troubles arise when running the pipeline.

## Database schema
There will be two staging tables and five analytic ones. The staging tables will ingest data from the JSON's in S3 with no transformation. The analytic database will follow a **star** schema, with a facts table called songplays, that will store song reproductions, and four dimensions.

### Fact Table:
- **songplays**: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

### Dimension Tables
- **users**: user_id, first_name, last_name, gender, level
- **songs**: song_id, title, artist_id, year, duration
- **artists**: artist_id, name, location, latitude, longitude
- **time**: start_time, hour, day, week, month, year, weekday

## Other files
There's also a dwh.cfg with the configuration parameters and keys to acces the redshift database.